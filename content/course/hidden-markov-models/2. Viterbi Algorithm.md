---
title: Viterbi Algorithm
date: '2023-04-25'
type: book
weight: 30

---


Viterbi Algorithm uses a ***Dynamic Programming*** paradigm to reduce the algorithm's time complexity by avoiding doing the same computation more than once.



## Computing the $\delta(s)$ distribution over states $\{ S_1, S_2 \ldots\}$

Let's introduce an equation:

$$ \delta_{S_j}(s + 1) = \underbrace{\underset{S_i}{max}}_{(1)}$$ $$[\underbrace{\delta_{S_i}(s)}_{(2)} * \underbrace{P(S_j | S_i)}_{(3)} * \underbrace{P(e_{s+1} | S_j)}_{(4)}] \ldots (1)$$

*In the equation above . . .*

1. $S_i$ iterates over all the underlying states in the model $S_1, S_2, S_3 . . . $ etc.

2. We will return to this in a while.

3. ***transition probability*** : what is the probability that $state j$ comes right after $state i$?

4. ***emission probability***: what is the probability that, if the underlying state at the  $ (S + 1)^{th}$ step is actually $S_j$, then the emission would be the observed emission viz. $e_{s+1}$? In other words, what is the probability that state $S_j$ *generates* or "emits" the observed emission $e_{s+1}$?

At step $(s + 1)$, for every underlying state $S_j$, you can use this to compute $\delta_{S_j}(S+1)$.

**So the natural question then is: what is this $\delta$ value?**


For clarity:

$\delta_{S_k}(s)$ is a numerical value assigned to $S_k$ at the $s^{th}$ step. 


$\rightarrow$ **Don't get confused!** 

We are using undercase $s$ to denote the step number, whereas uppercase with a subscript e.g. $S_k$ denotes some possible underlying state as defined in the model.

*You might have noticed* that:

* for each underlying state $S_k$ ($\forall{k})$, 

* $\delta_{S_k}$ at each step $s$  $\rightarrow$ written as $\delta_{S_k}(s)$  $\rightarrow$ $\forall{s}$ except $s = 1$ i.e. the first step 

is actually computed recursively!

$$ \delta_{S_j}(s) = {\underset{S_i}{max}} [{\delta_{S_i}(s - 1)} * {P(S_j | S_i)} * {P(e_{s} | S_j)}]$$

For the first step, the distribution $\delta(1)$ over the set of underlying states:

$\delta_{S_1}(1) = ?$ <br>
$\delta_{S_2}(1) = ?$ <br>
$\delta_{S_3}(1) = ?$ <br>
$\vdots$

for the Viterbi algorithm is specified beforehand, and is called the ***Initial Probability Matrix***.

As mentioned in the Intro, it is one of the three parameters needed beforehand to make the Viterbi algorithm work. 

All subsequent $\delta(2)$, $\delta(3) \ldots$ are computed using $Eq.1$ given above.

**But how to *interpet* this $\delta$ value?**

$\delta(s)$ is a distribution of over the set of underlying states $S_1, S_2 . . .$ at $ step s$. 

$\delta_{S_j}(s)$ represents the ***probability of the highest probability path*** of reaching $State S_j$ at $step s$. 

Basically in short, it is highest possible probability value of $State S_j$ at $step s$. One way I like to represent it in notation is as below.

$\delta_{S_j}(s) = \underset{k}{max} P(path_k(S_j, step = s)$

So, this way, with the three parameters known:

1. Initial Probability Matrix
2. Transition Probability Matrix
3. Emission Probability Matrix

you can compute the subsequent $\delta$ distributions viz. $\delta(2), \delta(3), \delta(4) . . . $ all the way upto $\delta({|L|})$ using this formula.

(Where $|L|$ is the length of the observed sequence, equivalently the number of underlying steps for the sequence, equivalently the step number at the last step.)

## Backtracking: Computing Best Predecessor

Once all the $\delta(s)$ distributions have been computed $\forall{s}$, start from the last step. 

You have the distribution $\delta(|L|)$ i.e. the probability distribution over states at the last viz. $|L|^{th}$ step. 

Simply infer the underlying state at the last step to be the state with the highest probability in $\delta(|L|)$: 

$$ \widehat{state_{|L|}} = \underset{S_i}{argmax} \delta_{S_i}(|S|)$$

Lock it as decided. Now *backtrack* from there to infer the best (i.e. most probable) candidate for the state at the previous step. This is called the **best predecessor**. 

$\widehat{state_{s-1}} = \overbrace{\psi(s)}_\text{best predecessor to current step} = \underset{S_i}{argmax} [\delta_{S_i}(s-1) * P(\widehat{state_{s}}|S_i) * P(e_{s-1}|S_i)]$

And that is it! Backtrack all the way to the first step i.e. $\widehat{state_{1}}$ or equivalently $\psi(2)$ and you have will have "unfolded" the most likely underlying sequence of states which generated the observed sequence of emissions. 

## Conclusion

That wraps up this tutorial on the Viterbi Algorithm. 

Note that, the Viterbi algorithm can work only if the three parameters are known:

1. Initial Probability Matrix
2. Transition Probability Matrix
3. Emission Probability Matrix

If we have a labelled dataset, we can compute the three parameters from it, to use for decoding future test instances.

**But if a labelled dataset isn't available**:

We can use the **Baum-Welch algorithm** to **learn the HMM parameters** from **unlabelled data**. 
